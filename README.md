ğŸ§  Distributed Edge AI Inference System

A production-style edge-to-cloud AI inference pipeline where edge devices send sensor data to a backend that performs asynchronous ML inference and stores results.

This project demonstrates backend engineering, system design, and applied ML inference.

ğŸš€ Features

Edge device sensor data simulation

FastAPI backend for data ingestion

Asynchronous inference queue

Background worker for ML inference

SQLite persistence (sensor data + predictions)

Modular and scalable architecture

ğŸ—ï¸ Architecture
Edge Device â†’ FastAPI API â†’ Inference Queue â†’ Worker â†’ ML Inference â†’ Database

ğŸ§  Tech Stack

Python

FastAPI

Pydantic

SQLite

Queue-based async processing

Logging

ğŸ“‚ Structure
backend/
 â”œâ”€â”€ app/
 â”‚   â”œâ”€â”€ main.py
 â”‚   â”œâ”€â”€ database.py
 â”‚   â”œâ”€â”€ schemas.py
 â”‚   â”œâ”€â”€ queue.py
 â”‚   â””â”€â”€ ml/inference.py
 â””â”€â”€ worker.py
edge_client/
 â””â”€â”€ simulator.py

âš™ï¸ Run
pip install -r requirements.txt
uvicorn backend.app.main:app --reload
python backend/worker.py
python edge_client/simulator.py

ğŸ“Š Sample Response
{
  "status": "stored",
  "prediction": {
    "prediction": "Normal",
    "score": 0.67
  }
}

ğŸ¯ Key Learnings

Asynchronous backend design

Edge-to-cloud AI workflows

ML inference in production systems

Queue-based scalability patterns

ğŸ‘¤ Author

Mujtaba Ahmed
Final-year Electrical Engineering student | Edge AI & Distributed Systems